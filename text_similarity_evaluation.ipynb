{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLjq8P1XJgBb"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0OLVtRe4R74"
      },
      "outputs": [],
      "source": [
        "!pip install textdistance nltk rouge werpy bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nnXPL0wF4GYq"
      },
      "outputs": [],
      "source": [
        "import textdistance as td\n",
        "from nltk.translate import meteor_score\n",
        "from rouge import Rouge\n",
        "from nltk.translate import bleu_score\n",
        "import werpy\n",
        "from bert_score import score\n",
        "\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "pd.set_option(\"max_colwidth\",1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKjnEhDE5DrY"
      },
      "outputs": [],
      "source": [
        "def average_lst(lst):\n",
        "  return sum(lst) / len(lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5aAtzqKT5RO"
      },
      "source": [
        "#<font color='red'>Prepare predictions and testcases</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY_d1Vhy5Q_j"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_excel(\"test.xlsx\", engine=\"openpyxl\")\n",
        "df_predict = pd.read_excel(\"predict.xlsx\", engine=\"openpyxl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK9F6p4Hh78S"
      },
      "outputs": [],
      "source": [
        "df_test.columns = ['entities', 'value_type'] + [('o'+str(i+1)) for i in range(len(df_test.columns)-2)]\n",
        "df_predict.columns = ['entities'] + [('o'+str(i+1)) for i in range(len(df_predict.columns)-1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt3MvrBQ5wBP"
      },
      "outputs": [],
      "source": [
        "applicant_index = 0\n",
        "\n",
        "entitiy_list = df_test.iloc[:,applicant_index].tolist()\n",
        "value_type_list = df_test.iloc[:,applicant_index+1].tolist()\n",
        "test_list = df_test.iloc[:,applicant_index+2].tolist()\n",
        "\n",
        "predict_list = df_predict.iloc[:,applicant_index+1].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52oJmn7YMg2g"
      },
      "source": [
        "# <font color='lime'>Metric Doc</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN2fTBTOFzDk"
      },
      "source": [
        "## <font color='lime'>Token Based Algorithms</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX0qVR8WMk7B"
      },
      "source": [
        "### **Sørensen-Dice similarity :**\n",
        "The Sørensen–Dice coefficient measures the similarity between two sets or strings. (for each element in the set of words: 1=exact_match , 0=not_exact_match)\n",
        "\n",
        "**Formula**: 2*|A∩B|/|A|+|B|\n",
        "\n",
        "https://yassineelkhal.medium.com/the-complete-guide-to-string-similarity-algorithms-1290ad07c6b7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKsI2L_TF2PM"
      },
      "source": [
        "## <font color='lime'>Edit Based Algorithms</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw3bHWQQyCa-"
      },
      "source": [
        "### **Levenshtein Distance :**\n",
        "Levenshtein distance measures the minimum number of edit operations (insertions, deletions, or substitutions) required to transform one string into another.\n",
        "\n",
        "**Applicability**: It works well when comparing strings of different lengths.\n",
        "### **Jaro Similarity :**\n",
        "Jaro similarity measures the similarity between two strings based on matching characters and transpositions (swapping of characters) within a certain window.\n",
        "\n",
        "**Applicability**: Primarily suited for short strings, such as person names.\n",
        "\n",
        "https://yassineelkhal.medium.com/the-complete-guide-to-string-similarity-algorithms-1290ad07c6b7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaGXHTnOVMAi"
      },
      "source": [
        "## <font color='lime'>WER (Word Error Rate)</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXh1vvXKVPoC"
      },
      "source": [
        "WER measures the divergence between a candidate sentence and the reference by counting the minimum number of word-level edits (insertions, deletions, substitutions) needed to transform one into the other. Primarily used in automatic speech recognition tasks.\n",
        "\n",
        "### **How It Works :**\n",
        "Compares the candidate and reference sentences word by word.\n",
        "\n",
        "Calculates the edit distance (Levenshtein distance) normalized by the reference length.\n",
        "\n",
        "### **Formula :**\n",
        "WER = (inserted + deleted + substituted)/total words in reference\n",
        "\n",
        "https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbscfRdmF9nw"
      },
      "source": [
        "## <font color='lime'>BLEU</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgU8F_Rcynk9"
      },
      "source": [
        "\n",
        "BLEU measures the precision of word n-grams (phrases) between a predicted text and a reference text.\n",
        "It focuses on word overlap and doesn’t consider recall or semantic meaning.\n",
        "\n",
        "**Precision** => This metric measures the number of words in the Predicted Sentence that also occur in the Target Sentence.\n",
        "### **How is Bleu Score calculated?**\n",
        "**1)** The first step is to compute Precision scores for 1-grams through 4-grams. (n1, n2, n3, n4)\n",
        "\n",
        "**2)** Calculating **Geometric Average Precision Scores**. weights = (w1, w2, w3, w4)\n",
        "\n",
        "    formula = (n1)^w1 . (n2)^w2 . (n3)^w3 . (n4)^w4\n",
        "\n",
        "**3)** The third step is to compute a **Brevity Penalty**. this penalty penalizes sentences that are too short.\n",
        "\n",
        "    if (c > r):\n",
        "      brevity_penalty = 1\n",
        "    else :\n",
        "      brevity_penalty = e ^ (1-r/c)\n",
        "    \n",
        "    c is predicted length = number of words in the predicted sentence\n",
        "    r is target length = number of words in the target sentence\n",
        "\n",
        "this ensures that if you predict very few words, Brevity Penalty will be small.\n",
        "\n",
        "**4)** Finally, to calculate the Bleu Score, we multiply the Brevity Penalty with the Geometric Average of the Precision Scores.\n",
        "\n",
        "    bleu_score = (Brevity Penalty)*(Geometric Average Precision Scores)\n",
        "\n",
        "https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b\n",
        "_______________________________________________________________________\n",
        "_______________________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRnAZDgryvOX"
      },
      "source": [
        "## <font color='lime'>ROUGE</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPJWI6yrGA0K"
      },
      "source": [
        "ROUGE measures recall and precision of word n-grams and longest common subsequences between the predicted and referenced texts.\n",
        "\n",
        "**Recall** => it quantifies the proportion of words or phrases in the Reference Sentence that also appear in the Predicted Sentence.\n",
        "\n",
        "**Precision** => This metric measures the number of words or phrases in the Predicted Sentence that also occur in the Reference Sentence.\n",
        "\n",
        "**F-Measure** => Combines precision and recall to provide a balanced view. It’s often reported as the harmonic mean of precision and recall.\n",
        "\n",
        "ROUGE is mostly used for assessing automatic summarization and machine translation\n",
        "\n",
        "### **ROUGE Variants :**\n",
        "### ROUGE-N:\n",
        "Measures n-gram overlap (unigram, bigram, trigram, etc.).\n",
        "\n",
        "### ROUGE-L:\n",
        "Measures the longest matching sequence of words using Longest Common Subsequence (LCS). It considers word order. Example:\n",
        "\n",
        "**Longest Common Subsequence (LCS)** => ROUGE-L looks at the entire summary without caring about where lines break. It aims to find similar parts between the two versions.\n",
        "\n",
        "https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm0qhQXZKCHz"
      },
      "source": [
        "## <font color='lime'>METEOR</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kewB_Pcwyyhq"
      },
      "source": [
        "METEOR modifies the precision and recall computations, replacing them with a weighted F-score based on mapping unigrams and a penalty function for incorrect word order.\n",
        "\n",
        "### **step 1)** calculate Precision and Recall with matching words.\n",
        "\n",
        "**Precision** => float(matches_count) / translation_length\n",
        "\n",
        "**Recall** => float(matches_count) / reference_length\n",
        "\n",
        "### **step 2)** Calculate weighted F-score using a parameter \"alpha\"\n",
        "\n",
        "**Fmean (F-score)** => (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n",
        "\n",
        "### **step 3)** Calculate penalty using parameters \"gamma\" and \"beta\"\n",
        "\n",
        "**penalty** => gamma * (chunk_count / matches_count) ** beta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNgNc6Nqy0fR"
      },
      "source": [
        "## <font color='lime'>BertScore</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsgvjtOWGDm2"
      },
      "source": [
        "BERTScore leverages contextual embeddings from pre-trained BERT models to measure token similarity.\n",
        "\n",
        "### **How It Works :**\n",
        "BERTScore leverages pre-trained BERT embeddings.\n",
        "\n",
        "It computes cosine similarity between words in candidate and reference sentences.\n",
        "\n",
        "The resulting score reflects semantic similarity.\n",
        "\n",
        "https://haticeozbolat17.medium.com/text-summarization-how-to-calculate-bertscore-771a51022964"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-OeCX5IqmJ3"
      },
      "source": [
        "#<font color='BLUE'>Exact Match</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyrFPXxuqoV4"
      },
      "outputs": [],
      "source": [
        "# position sensitive\n",
        "def compare_lists_exact_match(entities, value_types, predictions, references):\n",
        "  results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"exact_match\"])\n",
        "  for i in range(len(predictions)):\n",
        "    my_predict_str = str(predictions[i]).lower().split()\n",
        "    my_reference_str = str(references[i]).lower().split()\n",
        "    res = sum(p == r for p, r in zip(my_predict_str, my_reference_str))/max(len(my_reference_str),len(my_predict_str))\n",
        "    results.loc[len(results)] = [\n",
        "        entities[i],\n",
        "        value_types[i],\n",
        "        my_reference_str,\n",
        "        my_predict_str,\n",
        "        res\n",
        "        ]\n",
        "  avg = average_lst(results[\"exact_match\"].tolist())\n",
        "  return results, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-3wpyqHw-4y"
      },
      "outputs": [],
      "source": [
        "# position insensitive\n",
        "def compare_lists_sorensen(entities, value_types, predictions, references):\n",
        "  results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"sorensen_score\"])\n",
        "  for i in range(len(predictions)):\n",
        "    my_predict_str = str(predictions[i]).lower().split()\n",
        "    my_reference_str = str(references[i]).lower().split()\n",
        "    res = td.sorensen(my_predict_str, my_reference_str)\n",
        "    # match_count = 0\n",
        "    # for word_lower in my_predict_str:\n",
        "    #     if word_lower in my_reference_str:\n",
        "    #         my_reference_str.remove(word_lower)\n",
        "    #         match_count += 1\n",
        "    # res = match_count/len(my_reference_str)\n",
        "\n",
        "    results.loc[len(results)] = [\n",
        "          entities[i],\n",
        "          value_types[i],\n",
        "          my_reference_str,\n",
        "          my_predict_str,\n",
        "          res\n",
        "          ]\n",
        "  avg = average_lst(results[\"sorensen_score\"].tolist())\n",
        "  return results, avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbeP4RExTw0n"
      },
      "source": [
        "#<font color='yellow'>Text Distance</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFBdFUlmTvzu"
      },
      "outputs": [],
      "source": [
        "def compare_lists_jaro(entities, value_types, predictions, references):\n",
        "  results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"jaro_score\"])\n",
        "  for i in range(len(predictions)):\n",
        "    my_predict_str = str(predictions[i])\n",
        "    my_reference_str = str(references[i])\n",
        "    res = td.jaro(my_predict_str.lower(), my_reference_str.lower())\n",
        "    #res = sum(td.jaro(p.lower(), r.lower()) for p, r in zip(my_predict_str, my_reference_str))/len(my_reference_str)\n",
        "    results.loc[len(results)] = [\n",
        "        entities[i],\n",
        "        value_types[i],\n",
        "        my_reference_str,\n",
        "        my_predict_str,\n",
        "        round(res,4)\n",
        "        ]\n",
        "  avg = average_lst(results[\"jaro_score\"].tolist())\n",
        "  return results, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd3zl_HZNaIp"
      },
      "outputs": [],
      "source": [
        "def compare_lists_levenshtein(entities, value_types, predictions, references):\n",
        "  results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"levenshtein_score\"])\n",
        "  for i in range(len(predictions)):\n",
        "    my_predict_str = str(predictions[i])\n",
        "    my_reference_str = str(references[i])\n",
        "    res = td.levenshtein(my_predict_str.lower(), my_reference_str.lower())\n",
        "    # res = sum(td.levenshtein(p.lower(), r.lower()) for p, r in zip(my_predict_str, my_reference_str))/len(my_reference_str)\n",
        "    results.loc[len(results)] = [\n",
        "        entities[i],\n",
        "        value_types[i],\n",
        "        my_reference_str,\n",
        "        my_predict_str,\n",
        "        round(res,4)\n",
        "        ]\n",
        "  avg = average_lst(results[\"levenshtein_score\"].tolist())\n",
        "  return results, avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsTWmbQHaBLP"
      },
      "source": [
        "#<font color='pink'>Cosine similarity</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAMkAybyarHe"
      },
      "outputs": [],
      "source": [
        "# def compare_lists_cosine(entities, value_types, predictions, references):\n",
        "#   results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"cosine_score\"])\n",
        "#   for i in range(len(predictions)):\n",
        "#     my_predict_str = str(predictions[i])\n",
        "#     my_reference_str = str(references[i])\n",
        "#     r = td.cosine(my_predict_str.lower().split(), my_reference_str.lower().split())\n",
        "#     results.loc[len(results)] = [\n",
        "#         entities[i],\n",
        "#         value_types[i],\n",
        "#         my_reference_str,\n",
        "#         my_predict_str,\n",
        "#         round(r,4)\n",
        "#         ]\n",
        "#   avg = average_lst(results[\"cosine_score\"].tolist())\n",
        "#   return results, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0LA9baPayYV"
      },
      "outputs": [],
      "source": [
        "# res , avg = compare_lists_cosine(predict_list, test_list)\n",
        "# res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZlMMjDxJrSa"
      },
      "source": [
        "#<font color='LemonChiffon'>WER (Word Error Rate)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6j_klIKJt_r"
      },
      "outputs": [],
      "source": [
        "def compare_lists_wer(entities, value_types, predictions, references):\n",
        "  results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"wer_score\"])\n",
        "  for i in range(len(predictions)):\n",
        "    my_predict_str = str(predictions[i]).lower()\n",
        "    my_reference_str = str(references[i]).lower()\n",
        "    wer = werpy.wer(my_reference_str, my_predict_str)\n",
        "    results.loc[len(results)] = [\n",
        "        entities[i],\n",
        "        value_types[i],\n",
        "        my_reference_str,\n",
        "        my_predict_str,\n",
        "        round(wer,4)\n",
        "        ]\n",
        "  avg = average_lst(results[\"wer_score\"].tolist())\n",
        "  return results, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH80H_gfR6cA"
      },
      "outputs": [],
      "source": [
        "# results,_ = compare_lists_wer(entitiy_list, value_type_list, predict_list, test_list)\n",
        "# results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DC0nXN89R0sX"
      },
      "source": [
        "### Example :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp7BiPIJbfI8"
      },
      "outputs": [],
      "source": [
        "# reference = \"The cat is sleeping on the mat.\"\n",
        "# hypothesis = \"The cat is playing on mat.\"\n",
        "\n",
        "# # Calculate the overall Word Error Rate\n",
        "# wer_result = werpy.wer(reference, hypothesis)\n",
        "# print(f\"Word Error Rate: {wer_result:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv8Zb8OaTpFP"
      },
      "source": [
        "#<font color='orange'>BLEU</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RWaNWZJ5B7S"
      },
      "outputs": [],
      "source": [
        "def calculate_bleu(my_reference_str ,my_predict_str):\n",
        "  # step 1) Precision scores for 1-grams through 4-grams.\n",
        "  bleu_score_1gram = bleu_score.sentence_bleu([my_reference_str], my_predict_str,(1,0,0,0))\n",
        "  bleu_score_2gram = bleu_score.sentence_bleu([my_reference_str], my_predict_str,(0,1,0,0))\n",
        "  bleu_score_3gram = bleu_score.sentence_bleu([my_reference_str], my_predict_str,(0,0,1,0))\n",
        "  bleu_score_4gram = bleu_score.sentence_bleu([my_reference_str], my_predict_str,(0,0,0,1))\n",
        "  bleu_precisions = [round(bleu_score_1gram,4), round(bleu_score_2gram,4), round(bleu_score_3gram,4), round(bleu_score_4gram,4)]\n",
        "\n",
        "  r = len(my_reference_str)\n",
        "  c = len(my_predict_str)\n",
        "  gram=0\n",
        "  bleu = 0.0\n",
        "  if(r==0 or c==0):\n",
        "    print(f\"length zero {i}th str\")\n",
        "    bleu_precisions = [None]*4\n",
        "  elif(c==1 or r==1):\n",
        "    bleu_precisions[1:] = [None]*3\n",
        "    gram=1\n",
        "  elif(c==2 or r==2):\n",
        "    bleu_precisions[2:] = [None]*2\n",
        "    gram=2\n",
        "  elif(c==3 or r==3):\n",
        "    bleu_precisions[3] = None\n",
        "    gram=3\n",
        "  else:\n",
        "    gram=4\n",
        "  # step 2) geometric_average_precision_scores\n",
        "  weights = [1/gram]*gram\n",
        "  geometric_average_precision_scores = sum((n**w) for n,w in zip(bleu_precisions[:gram] , weights))\n",
        "  # step 3) brevity_penalty\n",
        "  brevity_penalty = 1\n",
        "  if (c <= r):\n",
        "    brevity_penalty = math.exp(1-(r/c))\n",
        "  # step 4) bleu_score\n",
        "  bleu = geometric_average_precision_scores * brevity_penalty\n",
        "  return bleu, bleu_precisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8OPt9v_LG8C"
      },
      "outputs": [],
      "source": [
        "from nltk.translate import bleu_score\n",
        "def compare_lists_bleu(entities, value_types, predictions, references):\n",
        "  compare_df = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"bleu_score\",\"bleu_precisions\"])\n",
        "  results = []\n",
        "  for i in range(len(predictions)):\n",
        "    my_predict_str = str(predictions[i]).split()\n",
        "    my_reference_str = str(references[i]).split()\n",
        "    try:\n",
        "      bleu, bleu_precisions = calculate_bleu(my_reference_str ,my_predict_str)\n",
        "      best_bleu_possible, _ = calculate_bleu(my_reference_str ,my_reference_str)\n",
        "    except:\n",
        "      print(\"Problem with bleu!!!!!!!!!!!!!!\")\n",
        "      pass\n",
        "\n",
        "    row = [entities[i], value_types[i], references[i], predictions[i], round(bleu/best_bleu_possible,4), bleu_precisions]\n",
        "    compare_df.loc[len(compare_df)] = row\n",
        "\n",
        "  # print(compare_df[\"bleu_score\"])\n",
        "  avg = average_lst(compare_df[\"bleu_score\"].tolist())\n",
        "  return compare_df, avg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "praud-k6Olue"
      },
      "outputs": [],
      "source": [
        "# bleu_df,_ = compare_lists_bleu(entitiy_list, value_type_list, predict_list, test_list)\n",
        "# bleu_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QEAE89dRus3"
      },
      "source": [
        "### Example :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw9TMAOk7gGu"
      },
      "outputs": [],
      "source": [
        "# from nltk.translate.bleu_score import corpus_bleu\n",
        "# from nltk.translate import bleu_score\n",
        "# ref = 'my first correct sentence'\n",
        "# can = 'my sentence'\n",
        "\n",
        "# score = bleu_score.corpus_bleu([ref.split()], [can.split()])\n",
        "# print(\"a\",score)\n",
        "# score = bleu_score.sentence_bleu([ref.split()], can.split())\n",
        "# print(\"b\",score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tyv40_B8mnFT"
      },
      "source": [
        "#<font color='aqua'>ROUGE</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4MO-nyGjLKY"
      },
      "outputs": [],
      "source": [
        "# def compare_lists_rouge(entities, value_types, predictions, references):\n",
        "#   scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "#   results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"rouge_precision\",\"rouge_recall\",\"rouge_f1\"])\n",
        "#   for i in range(len(predictions)):\n",
        "#     my_predict_str = str(predictions[i])\n",
        "#     my_reference_str = str(references[i])\n",
        "#     scores = scorer.score(my_predict_str.lower(), my_reference_str.lower())\n",
        "#     results.loc[len(results)] = [\n",
        "#         entities[i],\n",
        "#         value_types[i],\n",
        "#         my_reference_str,\n",
        "#         my_predict_str,\n",
        "#         round(scores['rougeL'].precision,4),\n",
        "#         round(scores['rougeL'].recall,4),\n",
        "#         round(scores['rougeL'].fmeasure,4)\n",
        "#         ]\n",
        "#   avg = average_lst(results[\"rouge_precision\"].tolist())\n",
        "#   return results, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKGLre4XwpXK"
      },
      "outputs": [],
      "source": [
        "def compare_lists_rouge(entities, value_types, predictions, references):\n",
        "  rouge = Rouge()\n",
        "  results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"rouge_lcs_recall\",\"rouge_lcs_precision\",\"rouge_lcs_f1\"])\n",
        "  for i in range(len(predictions)):\n",
        "    my_predict_str = str(predictions[i]).lower()\n",
        "    my_reference_str = str(references[i]).lower()\n",
        "    scores = rouge.get_scores(my_predict_str, my_reference_str)\n",
        "    results.loc[len(results)] = [\n",
        "        entities[i],\n",
        "        value_types[i],\n",
        "        my_reference_str,\n",
        "        my_predict_str,\n",
        "        round(scores[0]['rouge-l']['r'],4),\n",
        "        round(scores[0]['rouge-l']['p'],4),\n",
        "        round(scores[0]['rouge-l']['f'],4)\n",
        "        ]\n",
        "  avg = average_lst(results[\"rouge_lcs_f1\"].tolist())\n",
        "  return results, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvOeERSYkBmy"
      },
      "outputs": [],
      "source": [
        "# results, avg = compare_lists_rouge(entitiy_list, value_type_list, predict_list, test_list)\n",
        "# results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJY89VjqRo7H"
      },
      "source": [
        "### Example :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78ol6eGQwIgc"
      },
      "outputs": [],
      "source": [
        "# # Candidate and reference sentences\n",
        "# candidate = \"the cat sat on\"\n",
        "# reference = \"dog on the mat sat the cat\"\n",
        "\n",
        "# # Initialize ROUGE scorer\n",
        "# rouge = Rouge()\n",
        "\n",
        "# # Compute ROUGE scores\n",
        "# scores = rouge.get_scores(candidate, reference)\n",
        "# print(scores)\n",
        "# print(f\"ROUGE-1 F1 Score: {scores[0]['rouge-l']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn5XxrA-JJY1"
      },
      "source": [
        "#<font color='MediumSeaGreen'>METEOR</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz7C2S6e0o3k",
        "outputId": "b3db6fa8-7234-4d2c-8aa7-29de69775fe7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 216,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GFrwrEwQyVX"
      },
      "outputs": [],
      "source": [
        "def compare_lists_meteor(entities, value_types, predictions, references):\n",
        "  results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"meteor_f_score\"])\n",
        "  for i in range(len(predictions)):\n",
        "    # print('hi')\n",
        "    my_predict_str = str(predictions[i]).lower()\n",
        "    my_reference_str = str(references[i]).lower()\n",
        "    # print(\"predict : \",my_predict_str)\n",
        "    # print(\"reference : \",my_reference_str)\n",
        "    meteor = meteor_score.meteor_score([my_predict_str.split()], my_reference_str.split(), alpha=0.9)\n",
        "    best_meteor = meteor_score.meteor_score([my_reference_str.split()], my_reference_str.split(), alpha=0.9)\n",
        "    meteor_rate = None\n",
        "    if best_meteor != 0:\n",
        "      meteor_rate = meteor/best_meteor\n",
        "\n",
        "    results.loc[len(results)] = [\n",
        "        entities[i],\n",
        "        value_types[i],\n",
        "        my_reference_str,\n",
        "        my_predict_str,\n",
        "        round(meteor_rate,4)\n",
        "        ]\n",
        "  avg = average_lst(results[\"meteor_f_score\"].tolist())\n",
        "  return results, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD6sIq8ZQCh2"
      },
      "outputs": [],
      "source": [
        "# results, _ = compare_lists_meteor(entitiy_list, value_type_list, predict_list, test_list)\n",
        "# results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OZf5dEWRTYL"
      },
      "source": [
        "### Example :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsRSrGZUJbkc"
      },
      "outputs": [],
      "source": [
        "# candidate = \"the cat sat on the mat\"\n",
        "# reference = \"on the mat sat the cat\"\n",
        "\n",
        "# candidate = \"3815 brendan lane apt no. 3 north olmsted, oh 44070\"\n",
        "# reference = \"3815 brendan lane, apt no 3, north olmsted, oh, 44070\"\n",
        "\n",
        "# candidate = \"\"\n",
        "# reference = \"1\"\n",
        "# # Calculate METEOR score\n",
        "# meteor = meteor_score.meteor_score([candidate.split()], reference.split(), alpha=0.9)\n",
        "# best_meteor = meteor_score.meteor_score([reference.split()], reference.split(), alpha=0.9)\n",
        "# meteor_rate = None\n",
        "# if best_meteor != 0:\n",
        "#   meteor_rate = round(meteor/best_meteor,4)\n",
        "# print(\"METEOR Score:\",meteor_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-K9pd9hb_tN"
      },
      "source": [
        "#<font color='violet'>BERTscore</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkDbEo88smUi"
      },
      "outputs": [],
      "source": [
        "def compare_lists_bert_score(entities, value_types, predictions, references):\n",
        "  model = 'bert-base-uncased'\n",
        "  language = \"en\"\n",
        "  results = pd.DataFrame(columns = [\"entity\",\"value_type\",\"actual\",\"prediction\",\"bert_precision\",\"bert_recall\",\"bert_f1\"])\n",
        "  for i in range(len(predictions)):\n",
        "    my_predict_str = str(predictions[i]).lower()\n",
        "    my_reference_str = str(references[i]).lower()\n",
        "    bert_precision, bert_recall, bert_f1 = score([my_predict_str], [my_reference_str], lang=language, model_type=model)\n",
        "    results.loc[len(results)] = [\n",
        "        entities[i],\n",
        "        value_types[i],\n",
        "        my_reference_str,\n",
        "        my_predict_str,\n",
        "        np.round(bert_precision.mean().item(),4),\n",
        "        np.round(bert_recall.mean().item(),4),\n",
        "        np.round(bert_f1.mean().item(),4)\n",
        "        ]\n",
        "  avg = average_lst(results[\"bert_f1\"].tolist())\n",
        "  return results, avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJXoGvCFhC3a"
      },
      "outputs": [],
      "source": [
        "# results, avg = compare_lists_bert_score(entitiy_list, value_type_list, predict_list, test_list)\n",
        "# results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs5DMqE9RbHC"
      },
      "source": [
        "### Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5LHM_XWs5Nl"
      },
      "outputs": [],
      "source": [
        "# candidate = [\"the cat sat on the mat\"]\n",
        "# reference = [\"on the mat sat the cat\"]\n",
        "\n",
        "# candidate = [\"Stars shine, and the sky is covered with a blue blanket.\"]\n",
        "# reference = [\"Stars shine, and the sky is adorned with a bright color.\"]\n",
        "\n",
        "# candidate  = [\"The cat is on the mat.\"]\n",
        "# reference = [\"The dog is at the door.\"]\n",
        "\n",
        "# candidate = [\"The cat is sleeping.\"]\n",
        "# reference = [\"The cat rests.\"]\n",
        "\n",
        "# # Compute BERTScore\n",
        "# bert_precision, bert_recall, bert_f1 = score(candidate, reference, lang=\"en\", model_type='bert-base-uncased')\n",
        "# print(bert_f1.mean().item())\n",
        "# print(f\"BERTScore: {bert_f1.mean().item():.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d5aAtzqKT5RO",
        "52oJmn7YMg2g",
        "TN2fTBTOFzDk",
        "JKsI2L_TF2PM",
        "PaGXHTnOVMAi",
        "HbscfRdmF9nw",
        "QRnAZDgryvOX",
        "dm0qhQXZKCHz",
        "JNgNc6Nqy0fR",
        "h-OeCX5IqmJ3",
        "AbeP4RExTw0n",
        "qsTWmbQHaBLP",
        "YZlMMjDxJrSa",
        "DC0nXN89R0sX",
        "gv8Zb8OaTpFP",
        "3QEAE89dRus3",
        "Tyv40_B8mnFT",
        "zJY89VjqRo7H",
        "wn5XxrA-JJY1",
        "3OZf5dEWRTYL",
        "B-K9pd9hb_tN",
        "Zs5DMqE9RbHC"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}